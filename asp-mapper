#!/usr/bin/env python3
"""
asp-mapper.py
==================

Scanner gen√©rico para mapear endpoints ASP (ou similares) que seguem o padr√£o:
    [prefix][NNN][suffix].asp
Exemplo: /ace/ace001s.asp, /ace/ace002a.asp, /ace003d.asp

Pode ser usado para descobrir p√°ginas ativas, identificar erros 500,
hashes id√™nticos e comportamentos suspeitos em aplica√ß√µes legadas.

 Recursos:
- Configura√ß√£o completa via linha de comando
- Delay e concorr√™ncia ajust√°veis
- Salva resultados em CSV e JSON
- Gera hash SHA1 para detectar p√°ginas id√™nticas
- Extrai t√≠tulo HTML e snippet do corpo
- Suporte a cookies, headers e proxy

üîß Exemplo:
    python3 asp-mapper.py \
        --host example.com --prefix ace --start 1 --end 200 --suf a s d c \
        --delay 0.5 --workers 3 --allow-redirects \
        --headers "User-Agent: Mozilla/5.0; Accept: text/html" \
        --cookies "sessionid=abcd1234" \
        --out result.csv --json result.json
"""

import argparse
import requests
import hashlib
import time
import csv
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin
from typing import Dict, Optional

# ---------- Configura√ß√£o padr√£o ----------
DEFAULT_HEADERS = {
    "User-Agent": "endpoint-mapper/1.0 (+https://github.com/youruser)",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.8",
    "Connection": "keep-alive",
}

# ---------- Fun√ß√µes utilit√°rias ----------
def parse_kv_string(s: Optional[str]) -> Dict[str, str]:
    """Parses 'Header1: v1; Header2: v2' ou 'a=1; b=2' para dict."""
    if not s:
        return {}
    out = {}
    for part in [p.strip() for p in s.split(";") if p.strip()]:
        if ":" in part:
            k, v = part.split(":", 1)
        elif "=" in part:
            k, v = part.split("=", 1)
        else:
            continue
        out[k.strip()] = v.strip()
    return out

def build_url(base: str, prefix: str, num: int, width: int, suffix: str):
    num_str = str(num).zfill(width)
    filename = f"{prefix}{num_str}{suffix}.asp"
    path = f"/{prefix}/{filename}"
    return urljoin(base, path), path

def safe_request(session, url, headers=None, cookies=None, timeout=15, allow_redirects=False, proxy=None):
    try:
        resp = session.get(url, headers=headers, cookies=cookies, timeout=timeout,
                           allow_redirects=allow_redirects, proxies=proxy)
        return resp, None
    except requests.RequestException as e:
        return None, str(e)

def extract_meta(resp: requests.Response) -> Dict[str, str]:
    """Extrai t√≠tulo, snippet, hash e metadados do corpo."""
    body = resp.content or b""
    try:
        text = body.decode(resp.apparent_encoding or 'latin-1', errors='ignore')
    except Exception:
        text = ""
    title, snippet = "", text.strip().replace("\n", " ")[:300]
    try:
        lower = text.lower()
        if "<title" in lower:
            start = lower.find("<title")
            gt = lower.find(">", start)
            end = lower.find("</title>", gt)
            if gt != -1 and end != -1:
                title = text[gt+1:end].strip()
    except Exception:
        pass
    sha1 = hashlib.sha1(body).hexdigest()
    return {
        "status_code": resp.status_code,
        "len": len(body),
        "title": title,
        "snippet": snippet,
        "sha1": sha1,
        "url": resp.url,
        "redirected": bool(resp.history),
    }

def worker(session, base_url, prefix, num, width, suffix, headers, cookies, timeout, allow_redirects, proxy):
    url, path = build_url(base_url, prefix, num, width, suffix)
    resp, err = safe_request(session, url, headers, cookies, timeout, allow_redirects, proxy)
    if err:
        return {"path": path, "url": url, "error": err}
    meta = extract_meta(resp)
    return {
        "path": path,
        "url": url,
        "status_code": meta["status_code"],
        "content_length": meta["len"],
        "title": meta["title"],
        "snippet": meta["snippet"],
        "sha1": meta["sha1"],
        "redirected": meta["redirected"],
        "final_url": meta["url"],
    }

# ---------- Programa principal ----------
def main():
    parser = argparse.ArgumentParser(description="Mapeador gen√©rico de endpoints ASP")
    parser.add_argument("--host", required=True, help="Host alvo (ex: example.com)")
    parser.add_argument("--scheme", default="https", choices=["http","https"])
    parser.add_argument("--prefix", default="ace", help="Prefixo/pasta (ex: ace)")
    parser.add_argument("--start", type=int, default=1)
    parser.add_argument("--end", type=int, default=200)
    parser.add_argument("--width", type=int, default=3, help="Zeros √† esquerda (001, 002, ...)")
    parser.add_argument("--suf", nargs="+", default=["s","a","d","c","f"], help="Lista de sufixos (ex: s a d)")
    parser.add_argument("--delay", type=float, default=0.5, help="Delay entre requests por worker (s)")
    parser.add_argument("--workers", type=int, default=2, help="N√∫mero de threads")
    parser.add_argument("--out", default="map.csv", help="Arquivo CSV de sa√≠da")
    parser.add_argument("--json", default="map.json", help="Arquivo JSON de sa√≠da")
    parser.add_argument("--headers", default=None, help="Headers extras ('Header: valor; ...')")
    parser.add_argument("--cookies", default=None, help="Cookies ('a=1; b=2')")
    parser.add_argument("--timeout", type=int, default=15)
    parser.add_argument("--allow-redirects", action="store_true", default=False)
    parser.add_argument("--proxy", default=None, help="Proxy (http://127.0.0.1:8080)")
    args = parser.parse_args()

    base_url = f"{args.scheme}://{args.host}/"
    headers = DEFAULT_HEADERS.copy()
    headers.update(parse_kv_string(args.headers))
    cookies = parse_kv_string(args.cookies)
    proxies = {"http": args.proxy, "https": args.proxy} if args.proxy else None

    print(f"[+] Scanning {base_url} ({args.prefix}{args.start:03d}-{args.end:03d})")
    print(f"    Threads={args.workers} Delay={args.delay}s Proxy={args.proxy or 'None'}")

    session = requests.Session()
    session.headers.update(headers)
    results, hashes = [], {}

    try:
        with ThreadPoolExecutor(max_workers=args.workers) as exe:
            futures = []
            for n in range(args.start, args.end + 1):
                for suf in args.suf:
                    futures.append(exe.submit(worker, session, base_url, args.prefix, n, args.width, suf,
                                               headers, cookies, args.timeout, args.allow_redirects, proxies))
                    time.sleep(args.delay / max(1, args.workers))
            for fut in as_completed(futures):
                r = fut.result()
                results.append(r)
                sha = r.get("sha1")
                if sha:
                    hashes.setdefault(sha, []).append(r["path"])
                if r.get("error"):
                    print(f"[ERR] {r['path']} -> {r['error']}")
                else:
                    print(f"[{r['status_code']}] {r['path']} ({r['content_length']} bytes) {r['title'][:60]}")
    except KeyboardInterrupt:
        print("\n[!] Interrompido pelo usu√°rio")

    # CSV
    csv_fields = ["path","url","status_code","content_length","title","snippet","sha1","redirected","final_url","error"]
    with open(args.out, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=csv_fields)
        writer.writeheader()
        for r in results:
            writer.writerow({k: r.get(k, "") for k in csv_fields})
    print(f"[i] CSV salvo em {args.out}")

    # JSON
    with open(args.json, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"[i] JSON salvo em {args.json}")

    # Hash duplicates
    print("\n=== P√°ginas id√™nticas ===")
    for sha, paths in hashes.items():
        if len(paths) > 1:
            print(f"sha1={sha} ({len(paths)} p√°ginas): {paths[:10]}")

if __name__ == "__main__":
    main()
